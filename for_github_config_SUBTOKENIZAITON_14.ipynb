{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99fed51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "CONFIG_DICT = {\n",
    "    \"file_name\": \"litlat_14_github\",\n",
    "    \"bert\": \"EMBEDDIA/litlat-bert\", #AiLab-IMCS-UL/lvbert\n",
    "    \"bert_hidden_states\": True,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"lstm_hidden_size\": 1024,\n",
    "    \"lstm_num_layers\": 1,\n",
    "    \"lstm_bidirectional\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"dropout\": 0.25,\n",
    "    \"epochs\": 30,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd74ac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchtext\n",
      "Version: 0.6.0\n",
      "Summary: Text utilities and datasets for PyTorch\n",
      "Home-page: https://github.com/pytorch/text\n",
      "Author: PyTorch core devs and James Bradbury\n",
      "Author-email: jekbradbury@gmail.com\n",
      "License: BSD\n",
      "Location: c:\\users\\artur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\n",
      "Requires: numpy, requests, sentencepiece, six, torch, tqdm\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\artur\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# torchtext version 0.6.0 was used\n",
    "!pip show torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397681fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext import data, datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import functools\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40a55ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Seeding for result recreation\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Determine computing device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac22490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer for preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG_DICT['bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0899304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "# Initialize special token variables\n",
    "init_token = tokenizer.cls_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f282e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 3\n"
     ]
    }
   ],
   "source": [
    "# Store special token ids\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc66b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xlm-roberta-base': 512, 'xlm-roberta-large': 512, 'xlm-roberta-large-finetuned-conll02-dutch': 512, 'xlm-roberta-large-finetuned-conll02-spanish': 512, 'xlm-roberta-large-finetuned-conll03-english': 512, 'xlm-roberta-large-finetuned-conll03-german': 512}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.max_model_input_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dfe4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standart BERT max input\n",
    "max_input_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dce92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for tags\n",
    "def cut_and_convert_to_id(tokens, tokenizer, max_input_length):\n",
    "    tokens = tokens[:max_input_length-1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    tokens = tokens.lower()\n",
    "    tokens = tokenizer.tokenize(tokens)\n",
    "    tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return tokens\n",
    "\n",
    "def cut_to_max_length(tokens, max_input_length):\n",
    "    tokens = tokens[:max_input_length-1]\n",
    "    return tokens\n",
    "\n",
    "tag_preprocessor = functools.partial(cut_to_max_length,\n",
    "                                     max_input_length = max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1428acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Vārdšķira': <torchtext.data.field.Field object at 0x000002D761CC2560>, 'Pieturzīmes tips': <torchtext.data.field.Field object at 0x000002D761CC37F0>, 'Skaitlis': <torchtext.data.field.Field object at 0x000002D761CC3820>, 'Rekcija': <torchtext.data.field.Field object at 0x000002D761CC3850>, 'Locījums': <torchtext.data.field.Field object at 0x000002D761CC2CE0>, 'Dzimte': <torchtext.data.field.Field object at 0x000002D761CC2CB0>, 'Skaitlis 2': <torchtext.data.field.Field object at 0x000002D761CC15D0>, 'Lietvārda tips': <torchtext.data.field.Field object at 0x000002D761CC14E0>, 'Laiks': <torchtext.data.field.Field object at 0x000002D761CC2050>, 'Persona': <torchtext.data.field.Field object at 0x000002D761CC2080>, 'Darbības vārda tips': <torchtext.data.field.Field object at 0x000002D761CC20B0>, 'Izteiksme': <torchtext.data.field.Field object at 0x000002D761CC20E0>, 'Lokāmība': <torchtext.data.field.Field object at 0x000002D761CC2110>, 'Noteiktība': <torchtext.data.field.Field object at 0x000002D761CC2140>, 'Saīsinājuma tips': <torchtext.data.field.Field object at 0x000002D761CC2170>, 'Apstākļa vārda tips': <torchtext.data.field.Field object at 0x000002D761CC21A0>, 'Vietniekvārda tips': <torchtext.data.field.Field object at 0x000002D761CC36D0>, 'Reziduāļa tips': <torchtext.data.field.Field object at 0x000002D761CC36A0>}\n"
     ]
    }
   ],
   "source": [
    "# All possible Latvian language attributes\n",
    "TAG_NAMES = ['Vārdšķira',\n",
    " 'Pieturzīmes tips',\n",
    " 'Skaitlis',\n",
    " 'Rekcija',\n",
    " 'Locījums',\n",
    " 'Dzimte',\n",
    " 'Skaitlis 2',\n",
    " 'Lietvārda tips',\n",
    " 'Laiks',\n",
    " 'Persona',\n",
    " 'Darbības vārda tips',\n",
    " 'Izteiksme',\n",
    " 'Lokāmība',\n",
    " 'Noteiktība',\n",
    " 'Saīsinājuma tips',\n",
    " 'Apstākļa vārda tips',\n",
    " 'Vietniekvārda tips',\n",
    " 'Reziduāļa tips']\n",
    "\n",
    "# Initialize attribute dictionary\n",
    "tag_dict = {}\n",
    "\n",
    "# Populate dictionary\n",
    "tag_dict.update(dict(list(zip(TAG_NAMES, [data.Field(unk_token=None, init_token=\"<pad>\", preprocessing = tag_preprocessor) for x in TAG_NAMES]))))\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3023bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_id(tokens, tokenizer, max_input_length):\n",
    "    # List for subtokens to store\n",
    "    subtokens = []\n",
    "    # Subtoken mapping maps which word the subtoken belongs to\n",
    "    subtoken_map = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Convert token to only lower characters\n",
    "        token = token.lower()\n",
    "        # Get a list of all the subtokens for word\n",
    "        subtoken_list = tokenizer.tokenize(token)\n",
    "        # Add subtoken_list to all subtokens\n",
    "        subtokens.extend(subtoken_list)\n",
    "        # Add mapping for stored subtokens\n",
    "        subtoken_map.extend([i] * len(subtoken_list))\n",
    "\n",
    "    # Truncate if exceeds limit\n",
    "    subtokens = subtokens[:max_input_length - 1]\n",
    "    # Convert subtokens to numerical values\n",
    "    subtoken_ids = tokenizer.convert_tokens_to_ids(subtokens)\n",
    "\n",
    "    return subtoken_ids, subtoken_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8131d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tags_with_subtokens(tags, subtoken_map):\n",
    "    aligned_tags = []\n",
    "    # Track current tag\n",
    "    current_tag = \"\"\n",
    "    for index in subtoken_map:\n",
    "        # Check if tag is different from tag at index\n",
    "        if current_tag != tags[index]:\n",
    "            # Update current tag with tag at index\n",
    "            current_tag = tags[index]\n",
    "        aligned_tags.append(current_tag)\n",
    "    return aligned_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c848393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(sentence, tokenizer, max_input_length):\n",
    "    # Extract all words\n",
    "    text = [word[\"wordform\"] for word in sentence]\n",
    "    # Tokenize all words and convert them to ids, as well as get subtoken mapping\n",
    "    input_tokens, subtoken_map = tokenize_and_convert_to_id(text, tokenizer, max_input_length)\n",
    "\n",
    "    #Dictionary for storing tags\n",
    "    tag_dictionary = {}\n",
    "    \n",
    "    for field in TAG_NAMES:\n",
    "        # Extract all fields that are present in word's attributes\n",
    "        tags = [word[\"gold_attributes\"][field] if field in word[\"gold_attributes\"] else \"\" for word in sentence]\n",
    "        # Align the tags with subtoken mapping\n",
    "        tag_tokens = align_tags_with_subtokens(tags, subtoken_map)\n",
    "        # Store aligned tags\n",
    "        tag_dictionary[field] = tag_tokens\n",
    "\n",
    "    return input_tokens, list(tag_dictionary.values()), subtoken_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8999f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  JSON data files\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load necessary files\n",
    "train_file = load_json_file(\"train.json\")\n",
    "valid_file = load_json_file(\"dev.json\")\n",
    "test_file = load_json_file(\"test.json\")\n",
    "\n",
    "# Define text and subtoken mapping fields\n",
    "TEXT = data.Field(use_vocab=False, init_token=init_token_idx, pad_token=pad_token_idx, unk_token=unk_token_idx)\n",
    "# 769 value for padding is used to ensure it's out of sentence length borders\n",
    "ST_MAP = data.Field(use_vocab=False, init_token=init_token_idx, pad_token=769, unk_token=unk_token_idx)\n",
    "\n",
    "# Define each example case field tuple\n",
    "fields = tuple((zip([\"text\"] + [\"subtoken_map\"] + list(tag_dict.keys()), [TEXT] + [ST_MAP] + list(tag_dict.values()))))\n",
    "\n",
    "# Define examples for each file\n",
    "train_examples = []\n",
    "valid_examples = []\n",
    "test_examples = []\n",
    "test_old_examples = []\n",
    "tag_dictionary = {}\n",
    "\n",
    "# Populate tag_dictionary with attribute names\n",
    "tag_dictionary.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "\n",
    "# Load training examples\n",
    "for sentence in train_file:\n",
    "    input_tokens, tag_tokens, subtoken_map = preprocess_example(sentence, tokenizer, max_input_length)\n",
    "    train_examples.append(data.Example.fromlist([input_tokens, subtoken_map, *tag_tokens], fields))\n",
    "\n",
    "# Load validation examples\n",
    "for sentence in valid_file:\n",
    "    input_tokens, tag_tokens, subtoken_map = preprocess_example(sentence, tokenizer, max_input_length)\n",
    "    valid_examples.append(data.Example.fromlist([input_tokens, subtoken_map, *tag_tokens], fields))\n",
    "\n",
    "# Load test examples\n",
    "for sentence in test_file:\n",
    "    input_tokens, tag_tokens, subtoken_map = preprocess_example(sentence, tokenizer, max_input_length)\n",
    "    test_examples.append(data.Example.fromlist([input_tokens, subtoken_map, *tag_tokens], fields))\n",
    "\n",
    "# Create datasets\n",
    "train_data = data.Dataset(train_examples, fields)\n",
    "valid_data = data.Dataset(valid_examples, fields)\n",
    "test_data = data.Dataset(test_examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4149fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [3229, 643, 43922, 43036, 812, 24563, 265, 31, 47265, 933, 4, 2503, 39029, 408, 15, 13577, 14863, 5], 'subtoken_map': [0, 1, 2, 3, 4, 5, 5, 6, 7, 7, 8, 9, 10, 11, 11, 12, 12, 13], 'Vārdšķira': ['Pieturzīme', 'Prievārds', 'Lietvārds', 'Lietvārds', 'Darbības vārds', 'Darbības vārds', 'Darbības vārds', 'Saiklis', 'Darbības vārds', 'Darbības vārds', 'Pieturzīme', 'Pieturzīme', 'Darbības vārds', 'Saīsinājums', 'Saīsinājums', 'Lietvārds', 'Lietvārds', 'Pieturzīme'], 'Pieturzīmes tips': ['Pēdiņa', '', '', '', '', '', '', '', '', '', 'Komats', 'Pēdiņa', '', '', '', '', '', 'Punkts'], 'Skaitlis': ['', 'Daudzskaitlis', '', 'Vienskaitlis', 'Nepiemīt', 'Vienskaitlis', 'Vienskaitlis', '', 'Vienskaitlis', 'Vienskaitlis', '', '', 'Nepiemīt', '', '', 'Vienskaitlis', 'Vienskaitlis', ''], 'Rekcija': ['', 'Datīvs', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'Locījums': ['', '', 'Datīvs', 'Nominatīvs', '', 'Nominatīvs', 'Nominatīvs', '', 'Nominatīvs', 'Nominatīvs', '', '', '', '', '', 'Nominatīvs', 'Nominatīvs', ''], 'Dzimte': ['', '', 'Vīriešu', 'Vīriešu', '', 'Vīriešu', 'Vīriešu', '', 'Vīriešu', 'Vīriešu', '', '', '', '', '', 'Vīriešu', 'Vīriešu', ''], 'Skaitlis 2': ['', '', 'Daudzskaitlinieks', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'Lietvārda tips': ['', '', 'Sugas vārds', 'Sugas vārds', '', '', '', '', '', '', '', '', '', '', '', 'Īpašvārds', 'Īpašvārds', ''], 'Laiks': ['', '', '', '', 'Nākotne', 'Pagātne', 'Pagātne', '', 'Pagātne', 'Pagātne', '', '', 'Tagadne', '', '', '', '', ''], 'Persona': ['', '', '', '', '3', '', '', '', '', '', '', '', '3', '', '', '', '', ''], 'Darbības vārda tips': ['', '', '', '', \"Palīgverbs 'būt'\", 'Patstāvīgs darbības vārds', 'Patstāvīgs darbības vārds', '', 'Patstāvīgs darbības vārds', 'Patstāvīgs darbības vārds', '', '', 'Patstāvīgs darbības vārds', '', '', '', '', ''], 'Izteiksme': ['', '', '', '', 'Īstenības', 'Divdabis', 'Divdabis', '', 'Divdabis', 'Divdabis', '', '', 'Īstenības', '', '', '', '', ''], 'Lokāmība': ['', '', '', '', '', 'Lokāms', 'Lokāms', '', 'Lokāms', 'Lokāms', '', '', '', '', '', '', '', ''], 'Noteiktība': ['', '', '', '', '', 'Nenoteiktā', 'Nenoteiktā', '', 'Nenoteiktā', 'Nenoteiktā', '', '', '', '', '', '', '', ''], 'Saīsinājuma tips': ['', '', '', '', '', '', '', '', '', '', '', '', '', 'Īpašvārds', 'Īpašvārds', '', '', ''], 'Apstākļa vārda tips': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'Vietniekvārda tips': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'Reziduāļa tips': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a365baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'<pad>': 0, 'Lietvārds': 1, 'Darbības vārds': 2, 'Pieturzīme': 3, 'Īpašības vārds': 4, 'Vietniekvārds': 5, 'Apstākļa vārds': 6, 'Saiklis': 7, 'Prievārds': 8, 'Reziduālis': 9, 'Partikula': 10, 'Saīsinājums': 11, 'Skaitļa vārds': 12, 'Izsauksmes vārds': 13})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Komats': 2, 'Punkts': 3, 'Pēdiņa': 4, 'Domuzīme': 5, 'Iekava': 6, 'Kols': 7, 'Cita': 8})\n",
      "defaultdict(None, {'<pad>': 0, 'Vienskaitlis': 1, '': 2, 'Daudzskaitlis': 3, 'Nepiemīt': 4})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Akuzatīvs': 2, 'Datīvs': 3, 'Ģenitīvs': 4, 'Nepiemīt': 5})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Nominatīvs': 2, 'Ģenitīvs': 3, 'Akuzatīvs': 4, 'Datīvs': 5, 'Lokatīvs': 6, 'Nepiemīt': 7, 'Vokatīvs': 8})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Vīriešu': 2, 'Sieviešu': 3, 'Nepiemīt': 4})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Daudzskaitlinieks': 2, 'Vienskaitlinieks': 3})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Sugas vārds': 2, 'Īpašvārds': 3})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Pagātne': 2, 'Tagadne': 3, 'Nepiemīt': 4, 'Nākotne': 5})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, '3': 2, 'Nepiemīt': 3, '1': 4, '2': 5})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Patstāvīgs darbības vārds': 2, \"Palīgverbs 'būt'\": 3, 'Modāls': 4, \"Palīgverbi 'tikt' un 'tapt'\": 5, \"Saitiņas 'tikt' un 'tapt'\": 6, 'Fāzes': 7, 'Izpausmes veida': 8})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Īstenības': 2, 'Divdabis': 3, 'Nenoteiksme': 4, 'Vēlējuma': 5, 'Vajadzības': 6, 'Pavēles': 7, 'Atstāstījuma': 8})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Lokāms': 2, 'Nelokāms': 3, 'Daļēji lokāms': 4})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Nenoteiktā': 2, 'Noteiktā': 3, 'Nepiemīt': 4})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Īpašvārds': 2, 'Sugasvārds': 3, 'Diskursa iezīmētāji': 4, 'Apstāklis': 5, 'Īpašības vārds': 6, 'Verbāls': 7})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Veida': 2, 'Laika': 3, 'Mēra': 4, 'Vietas': 5, 'Cēloņa/nolūka': 6})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Norādāmais': 2, 'Personas': 3, 'Attieksmes': 4, 'Nenoteiktais': 5, 'Noteiktais': 6, 'Piederības': 7, 'Atgriezeniskais': 8, 'Jautājamais': 9})\n",
      "defaultdict(None, {'<pad>': 0, '': 1, 'Skaitlis cipariem': 2, 'Vārds svešvalodā': 3, 'Kārtas skaitlis cipariem': 4, 'Cits': 5, 'URI': 6})\n"
     ]
    }
   ],
   "source": [
    "for tag in tag_dict.keys():\n",
    "    tag_dict[tag].build_vocab(train_data)\n",
    "    print(tag_dict[tag].vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21d19f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Vārdšķira': <torchtext.data.field.Field object at 0x000002D761CC2560>, 'Pieturzīmes tips': <torchtext.data.field.Field object at 0x000002D761CC37F0>, 'Skaitlis': <torchtext.data.field.Field object at 0x000002D761CC3820>, 'Rekcija': <torchtext.data.field.Field object at 0x000002D761CC3850>, 'Locījums': <torchtext.data.field.Field object at 0x000002D761CC2CE0>, 'Dzimte': <torchtext.data.field.Field object at 0x000002D761CC2CB0>, 'Skaitlis 2': <torchtext.data.field.Field object at 0x000002D761CC15D0>, 'Lietvārda tips': <torchtext.data.field.Field object at 0x000002D761CC14E0>, 'Laiks': <torchtext.data.field.Field object at 0x000002D761CC2050>, 'Persona': <torchtext.data.field.Field object at 0x000002D761CC2080>, 'Darbības vārda tips': <torchtext.data.field.Field object at 0x000002D761CC20B0>, 'Izteiksme': <torchtext.data.field.Field object at 0x000002D761CC20E0>, 'Lokāmība': <torchtext.data.field.Field object at 0x000002D761CC2110>, 'Noteiktība': <torchtext.data.field.Field object at 0x000002D761CC2140>, 'Saīsinājuma tips': <torchtext.data.field.Field object at 0x000002D761CC2170>, 'Apstākļa vārda tips': <torchtext.data.field.Field object at 0x000002D761CC21A0>, 'Vietniekvārda tips': <torchtext.data.field.Field object at 0x000002D761CC36D0>, 'Reziduāļa tips': <torchtext.data.field.Field object at 0x000002D761CC36A0>}\n"
     ]
    }
   ],
   "source": [
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e3717f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b29397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize batch size from config\n",
    "BATCH_SIZE = CONFIG_DICT['batch_size']\n",
    "\n",
    "# Create data iterators from given datasets\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort=False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9abdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagger class\n",
    "class BERTPoSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dict,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Assign bert\n",
    "        self.bert = bert\n",
    "        \n",
    "        # Bert hidden size\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=CONFIG_DICT['lstm_hidden_size'],\n",
    "                            num_layers=CONFIG_DICT['lstm_num_layers'],\n",
    "                            bidirectional=CONFIG_DICT['lstm_bidirectional'],\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # All linear classification layer list\n",
    "        self.y = []\n",
    "        \n",
    "        # Store linear classification layer for each attribute\n",
    "        for dim_size in output_dict:\n",
    "            self.y.append(nn.Linear(CONFIG_DICT['lstm_hidden_size'] * 2 if CONFIG_DICT['lstm_bidirectional'] else CONFIG_DICT['lstm_hidden_size'], dim_size).to(device))\n",
    "        \n",
    "        # Define dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # Make sure input corresponds to used device\n",
    "        text.to(device)\n",
    "        # text = [sentence length, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "        # text = [batch size, sentence length]\n",
    "        \n",
    "        # Retrieve embeddings\n",
    "        embedded = self.dropout(self.bert(text)[1][-1])\n",
    "\n",
    "        # Retrieve LSTM output based on embeddings\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # lstm_out = [batch_size, sentence length, embedding dimensions]\n",
    "        \n",
    "        lstm_out = lstm_out.permute(1, 0, 2)\n",
    "        # lstm_out = [sentence length, batch_size, embedding dimensions]\n",
    "\n",
    "        # List for all outputs\n",
    "        output_list = []\n",
    "    \n",
    "        for y in self.y:\n",
    "            # Get classifications for each attribute\n",
    "            output_list.append(y(self.dropout(lstm_out)))\n",
    "        \n",
    "        return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2860e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/litlat-bert were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/litlat-bert and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(CONFIG_DICT['bert'], output_hidden_states=CONFIG_DICT['bert_hidden_states'])\n",
    "# Run BERT on device\n",
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa3053de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 9, 5, 6, 9, 5, 4, 4, 6, 6, 9, 9, 5, 5, 8, 7, 10, 7]\n"
     ]
    }
   ],
   "source": [
    "# Store output dimensions for each attribute\n",
    "output_dims = []\n",
    "for tag in tag_dict.keys():\n",
    "    output_dims.append(len(tag_dict[tag].vocab))\n",
    "\n",
    "# Check possible classes for each attribute\n",
    "print(output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efb2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dropout ratio\n",
    "DROPOUT = CONFIG_DICT['dropout']\n",
    "\n",
    "# Initialize tagger instance\n",
    "model = BERTPoSTagger(bert,\n",
    "                      output_dims,\n",
    "                      DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6640597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 165,406,466 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "057192d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), CONFIG_DICT['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bf9ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_list = []\n",
    "tag_pad_list = []\n",
    "for tag in tag_dict.keys():\n",
    "    # Populate paddings in case different paddings were used in preprocessing\n",
    "    tag_pad_idx = tag_dict[tag].vocab.stoi[tag_dict[tag].pad_token]\n",
    "    tag_pad_list.append(tag_pad_idx)\n",
    "    # Define criterion for each attribute\n",
    "    criterion_list.append(nn.CrossEntropyLoss(ignore_index = tag_pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "089b9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model and each criterion on device\n",
    "model = model.to(device)\n",
    "\n",
    "for criterion in criterion_list:\n",
    "    criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "659be1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return accuracy per batch\n",
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    # Get index of maximum probability\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    # Only take non padding elements into account\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    # Retrieve all correct predictions\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    # Return the ratio of correct predictions\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fd3f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion_list, tag_pad_list):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    epoch_acc_list = []\n",
    "    \n",
    "    # Initialize model training\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        loss = 0\n",
    "        accuracy_list = []\n",
    "        \n",
    "        batch_tag_dictionary = {}\n",
    "\n",
    "        batch_tag_dictionary.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "        \n",
    "        batch_dict = vars(batch)\n",
    "\n",
    "        # Retrieve text of batch\n",
    "        text = batch.text.to(device)\n",
    "        \n",
    "        # Populate batch_tag_dictionary with all tags from batch\n",
    "        for tag in batch_tag_dictionary.keys():\n",
    "            batch_tag_dictionary[tag] = batch_dict[tag].view(-1)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Retrieve predictions\n",
    "        predictions_list = model(text)\n",
    "\n",
    "        # Reshape predictions\n",
    "        for i in range(0, len(predictions_list)):\n",
    "            predictions_list[i] = predictions_list[i].view(-1, predictions_list[i].shape[-1])\n",
    "\n",
    "        # Calculate loss for each attribute\n",
    "        for i in range(0, len(predictions_list)):\n",
    "            loss += criterion_list[i](predictions_list[i], batch_tag_dictionary[TAG_NAMES[i]])\n",
    "\n",
    "        # Calculate accuracy for each attribute\n",
    "        for i in range(0, len(predictions_list)):\n",
    "            accuracy_list.append(categorical_accuracy(predictions_list[i], batch_tag_dictionary[TAG_NAMES[i]], tag_pad_list[i]))\n",
    "\n",
    "        # Backpropogation\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add batch loss to epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Add batch accuracy to epoch accuracy\n",
    "        epoch_acc_list.append([accuracy.item() for accuracy in accuracy_list])\n",
    "       \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate average los and accuracy for each epoch\n",
    "    return epoch_loss / len(iterator), [mean(x) for x in list(zip(*epoch_acc_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f460e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating whole tag accuracy:\n",
    "# NOTE: This is used only during training evakluation, as it calculates predictions of subtokens\n",
    "# For model test evaluation, a different method is used, due to computational resource saving\n",
    "def return_category_list(preds_list, y_list, tag_pad_idx_list):\n",
    "    temp_list = []\n",
    "    \n",
    "    dict_pred = {}\n",
    "    dict_pred.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "    dict_target = {}\n",
    "    dict_target.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "    \n",
    "    # Iterate over each attribute for predictions\n",
    "    for i, tag in enumerate(dict_pred.keys()):\n",
    "        # Retrieve non padding elements\n",
    "        non_pad_elements = (y_list[tag] != tag_pad_idx_list[i]).nonzero().view(-1)\n",
    "        dict_target[tag] = y_list[tag][non_pad_elements].view(-1)\n",
    "        # Retrieve maximum probability index for attribute's predictions\n",
    "        dict_pred[tag] = preds_list[i].argmax(dim = 1, keepdim = True)[non_pad_elements].squeeze(1) # get the index of the max probability\n",
    "   \n",
    "    true_cnt = 0\n",
    "    # Iterate over predictions\n",
    "    for i in range(0, len(dict_pred['Vārdšķira'])):\n",
    "        correct = 0\n",
    "        # Check if predicted value matches target value for each attribute\n",
    "        for j, tag in enumerate(dict_pred.keys()):\n",
    "            if dict_target[tag][i] == dict_pred[tag][i]:\n",
    "                correct = correct + 1\n",
    "        # If all predicted tags match the expected - add as correct tag\n",
    "        if correct == len(dict_pred):\n",
    "            true_cnt = true_cnt+1\n",
    "    \n",
    "    # Return the count of correctly predicted tags\n",
    "    return true_cnt/len(dict_pred['Vārdšķira'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d839cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for calculating whole tag accuracy:\n",
    "# NOTE: This is used only during testing evaluation, as it calculates accuracy of predictions\n",
    "# taking into account subtoken mapping.\n",
    "# This method assumes first subtoken prediction represents the prediction for the whole word\n",
    "def test_return_category_list(preds_list, y_list, tag_pad_idx_list, subtoken_mapping):\n",
    "    subtoken_mapping = subtoken_mapping.t()\n",
    "    counter = -1\n",
    "    true_element_counter = []\n",
    "    first_encounter = True\n",
    "    \n",
    "    # Iterate over subtoken mapping\n",
    "    for i in range(subtoken_mapping.shape[0]):\n",
    "        # If it's first encounter - it means it's the first subtoken\n",
    "        first_encounter = True\n",
    "        for j in range(subtoken_mapping.shape[1]):\n",
    "            # Skip sequence start tokens\n",
    "            if j == 0:\n",
    "                continue\n",
    "            # Mark token as traversed\n",
    "            counter = counter + 1\n",
    "            \n",
    "            # Ignore indices of padding elements\n",
    "            if subtoken_mapping[i, j].item() == 769:\n",
    "                counter = counter - 1\n",
    "                continue\n",
    "            # If it's first encounter - append this index as relevant for calculation\n",
    "            if first_encounter == True:\n",
    "                true_element_counter.append(counter)\n",
    "            # If next subtoken is same as current subtoken set first_encounter as false for next iteraiton\n",
    "            if j != subtoken_mapping.shape[1] - 1:\n",
    "                if subtoken_mapping[i, j+1] == subtoken_mapping[i, j]:\n",
    "                    first_encounter = False\n",
    "                else:\n",
    "                    first_encounter = True\n",
    "\n",
    "    temp_list = []\n",
    "    dict_subtoken_mapping = {}\n",
    "    \n",
    "    dict_pred = {}\n",
    "    dict_pred.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "    dict_target = {}\n",
    "    dict_target.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "\n",
    "    # Get non-padding predictions for each attribute\n",
    "    for i, tag in enumerate(dict_pred.keys()):\n",
    "        non_pad_elements = (y_list[tag] != tag_pad_idx_list[i]).nonzero().view(-1)\n",
    "\n",
    "        dict_target[tag] = y_list[tag][non_pad_elements].view(-1)\n",
    "\n",
    "        dict_pred[tag] = preds_list[i].argmax(dim = 1, keepdim = True)[non_pad_elements].squeeze(1) # get the index of the max probability\n",
    "    \n",
    "    # Iterate over each word and check if all attributes are predicted correctly\n",
    "    true_cnt = 0\n",
    "    for i in range(0, len(dict_pred['Vārdšķira'])):\n",
    "        correct = 0\n",
    "        for j, tag in enumerate(dict_pred.keys()):\n",
    "            # First condition skips elements that are not first subtokens\n",
    "            if i in true_element_counter and dict_target[tag][i] == dict_pred[tag][i]:\n",
    "                correct = correct + 1\n",
    "        if correct == len(dict_pred):\n",
    "            true_cnt = true_cnt+1\n",
    "\n",
    "    # Return the count of correctly tagged first subtokens\n",
    "    return true_cnt/len(true_element_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f959217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation used during training\n",
    "def evaluate(model, iterator, optimizer, criterion_list, tag_pad_list):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc_list = []\n",
    "    tag_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Only difference for evaluation - do not use gradient\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            loss = 0\n",
    "            accuracy_list = []\n",
    "\n",
    "            batch_tag_dictionary = {}\n",
    "\n",
    "            batch_tag_dictionary.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "\n",
    "            batch_dict = vars(batch)\n",
    "\n",
    "            text = batch.text\n",
    "\n",
    "            for tag in batch_tag_dictionary.keys():\n",
    "                batch_tag_dictionary[tag] = batch_dict[tag].view(-1)\n",
    "\n",
    "            predictions_list = model(text)\n",
    "\n",
    "            for i in range(0, len(predictions_list)):\n",
    "                predictions_list[i] = predictions_list[i].view(-1, predictions_list[i].shape[-1])\n",
    "            \n",
    "            for i in range(0, len(predictions_list)):\n",
    "                loss += criterion_list[i](predictions_list[i], batch_tag_dictionary[TAG_NAMES[i]])\n",
    "\n",
    "            # Utilize training evaluation tagging accuracy method\n",
    "            tag_accuracy.append(return_category_list(predictions_list, batch_tag_dictionary, tag_pad_list))\n",
    "\n",
    "            for i in range(0, len(predictions_list)):\n",
    "                accuracy_list.append(categorical_accuracy(predictions_list[i], batch_tag_dictionary[TAG_NAMES[i]], tag_pad_list[i]))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            epoch_acc_list.append([accuracy.item() for accuracy in accuracy_list])\n",
    "    \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    epoch_avg_acc_list = []\n",
    "\n",
    "    return epoch_loss / len(iterator), [mean(x) for x in list(zip(*epoch_acc_list))], tag_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "834a35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation used during testing\n",
    "def test_evaluate(model, iterator, optimizer, criterion_list, tag_pad_list):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc_list = []\n",
    "    tag_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            loss = 0\n",
    "            accuracy_list = []\n",
    "            eval_dict = {}\n",
    "\n",
    "            batch_tag_dictionary = {}\n",
    "            batch_tag_dictionary.update(dict(list(zip(TAG_NAMES, [None for x in TAG_NAMES]))))\n",
    "\n",
    "            batch_dict = vars(batch)\n",
    "\n",
    "            text = batch.text\n",
    "            subtoken_mapping = batch.subtoken_map\n",
    "\n",
    "            for tag in batch_tag_dictionary.keys():\n",
    "                batch_tag_dictionary[tag] = batch_dict[tag].view(-1)\n",
    "\n",
    "            predictions_list = model(text)\n",
    "\n",
    "            for i in range(0, len(predictions_list)):\n",
    "                predictions_list[i] = predictions_list[i].view(-1, predictions_list[i].shape[-1])\n",
    "            \n",
    "            for i in range(0, len(predictions_list)):\n",
    "                loss += criterion_list[i](predictions_list[i], batch_tag_dictionary[TAG_NAMES[i]])\n",
    "\n",
    "            # Utilize testing evaluation tagging accuracy method\n",
    "            tag_accuracy.append(test_return_category_list(predictions_list, batch_tag_dictionary, tag_pad_list, subtoken_mapping))\n",
    "\n",
    "            for i in range(0, len(predictions_list)):\n",
    "                accuracy_list.append(categorical_accuracy(predictions_list[i], batch_tag_dictionary[TAG_NAMES[i]], tag_pad_list[i]))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            epoch_acc_list.append([accuracy.item() for accuracy in accuracy_list])\n",
    "    \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    epoch_avg_acc_list = []\n",
    "  \n",
    "    return epoch_loss / len(iterator), [mean(x) for x in list(zip(*epoch_acc_list))], tag_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48d6b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c085aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Skip this cell to not train model\n",
    "# Epoch count\n",
    "N_EPOCHS = CONFIG_DICT['epochs']\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "datetime_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "file_name = CONFIG_DICT['file_name']\n",
    "\n",
    "# Variable to determine best epoch\n",
    "best_valid_epoch = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform model training\n",
    "    train_loss, train_acc_list = train(model, train_iterator, optimizer, criterion_list, tag_pad_list)\n",
    "    # Commence training evaluation\n",
    "    valid_loss, valid_acc_list, tag_accuracy_list = evaluate(model, valid_iterator, optimizer, criterion_list, tag_pad_list)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate epoch time\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Store best validation loss state and epoch\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_epoch = epoch+1\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}' )\n",
    "    for train_acc in train_acc_list:\n",
    "        print(f'Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    for val_acc in valid_acc_list:\n",
    "        print(f'Val. Acc: {val_acc*100:.2f}%')\n",
    "    print(f'Tag Acc: {mean(tag_accuracy_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Valid Epoch: {best_valid_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68f510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to load pretrained model or comment to use trained\n",
    "file_name = \"litlat_after_accuracy_15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a04d24a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best state (or pretrained model) before testing\n",
    "model.load_state_dict(torch.load(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3a27258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': 'litlat_14_github', 'bert': 'EMBEDDIA/litlat-bert', 'bert_hidden_states': True, 'learning_rate': 5e-05, 'lstm_hidden_size': 1024, 'lstm_num_layers': 1, 'lstm_bidirectional': True, 'batch_size': 32, 'dropout': 0.25, 'epochs': 30}\n",
      "Test Loss: 1.546\n",
      "Test Acc:Vārdšķira, 98.59%\n",
      "Test Acc:Pieturzīmes tips, 99.99%\n",
      "Test Acc:Skaitlis, 97.83%\n",
      "Test Acc:Rekcija, 99.75%\n",
      "Test Acc:Locījums, 98.49%\n",
      "Test Acc:Dzimte, 97.91%\n",
      "Test Acc:Skaitlis 2, 99.40%\n",
      "Test Acc:Lietvārda tips, 98.76%\n",
      "Test Acc:Laiks, 99.04%\n",
      "Test Acc:Persona, 99.47%\n",
      "Test Acc:Darbības vārda tips, 99.23%\n",
      "Test Acc:Izteiksme, 99.41%\n",
      "Test Acc:Lokāmība, 99.80%\n",
      "Test Acc:Noteiktība, 99.63%\n",
      "Test Acc:Saīsinājuma tips, 99.78%\n",
      "Test Acc:Apstākļa vārda tips, 99.26%\n",
      "Test Acc:Vietniekvārda tips, 99.77%\n",
      "Test Acc:Reziduāļa tips, 99.73%\n",
      "Tag Acc: 0.9311444874331951\n"
     ]
    }
   ],
   "source": [
    "print(CONFIG_DICT)\n",
    "\n",
    "# Evaluate model, using word representation by first subtoken\n",
    "test_loss, test_acc_list, tag_accuracy_list = test_evaluate(model, test_iterator, optimizer, criterion_list, tag_pad_list)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}')\n",
    "for i, test_acc in enumerate(test_acc_list):\n",
    "        print(f'Test Acc:{TAG_NAMES[i]}, {test_acc*100:.2f}%')\n",
    "print(f'Tag Acc: {mean(tag_accuracy_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "400ee3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(model, device, sentence, tokenizer, text_field, tag_field_list):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        tokens = sentence\n",
    "    \n",
    "    # Convert to ids\n",
    "    numericalized_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    numericalized_tokens = [text_field.init_token] + numericalized_tokens\n",
    "    \n",
    "    unk_idx = text_field.unk_token\n",
    "    \n",
    "    # Identify unknown tokens\n",
    "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
    "    \n",
    "    # Initialize token tensor\n",
    "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
    "    \n",
    "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions_list = model(token_tensor)\n",
    "    \n",
    "    # Retrieve top predictions\n",
    "    top_predictors_list = []\n",
    "    for i in range(0, len(predictions_list)):\n",
    "        top_predictors_list.append(predictions_list[i].argmax(-1))\n",
    "    \n",
    "    # Get all predicted tags\n",
    "    predicted_tags_list = []\n",
    "    for i, tag in enumerate(tag_field_list):\n",
    "        predicted_tags_list.append([tag_field_list[tag].vocab.itos[t.item()] for t in top_predictors_list[i]])\n",
    "        predicted_tags_list[i] = predicted_tags_list[i][1:]\n",
    "\n",
    "    # Assert prediction length matches token length\n",
    "    for predicted_tags in predicted_tags_list:\n",
    "        assert len(tokens) == len(predicted_tags)\n",
    "    \n",
    "    return tokens, predicted_tags_list, unks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb2575df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Brīdi turēja to rokās.'\n",
    "\n",
    "tokens, predicted_tags_list, unks = tag_sentence(model, \n",
    "                                  device, \n",
    "                                  sentence,\n",
    "                                  tokenizer,\n",
    "                                  TEXT,\n",
    "                                  tag_dict)\n",
    "\n",
    "print(unks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "819a7617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>Vārdšķira</th>\n",
       "      <th>Pieturzīmes tips</th>\n",
       "      <th>Skaitlis</th>\n",
       "      <th>Rekcija</th>\n",
       "      <th>Locījums</th>\n",
       "      <th>Dzimte</th>\n",
       "      <th>Skaitlis 2</th>\n",
       "      <th>Lietvārda tips</th>\n",
       "      <th>Laiks</th>\n",
       "      <th>Persona</th>\n",
       "      <th>Darbības vārda tips</th>\n",
       "      <th>Izteiksme</th>\n",
       "      <th>Lokāmība</th>\n",
       "      <th>Noteiktība</th>\n",
       "      <th>Saīsinājuma tips</th>\n",
       "      <th>Apstākļa vārda tips</th>\n",
       "      <th>Vietniekvārda tips</th>\n",
       "      <th>Reziduāļa tips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁Br</td>\n",
       "      <td>Lietvārds</td>\n",
       "      <td></td>\n",
       "      <td>Daudzskaitlis</td>\n",
       "      <td></td>\n",
       "      <td>Nominatīvs</td>\n",
       "      <td>Vīriešu</td>\n",
       "      <td></td>\n",
       "      <td>Sugas vārds</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>īdi</td>\n",
       "      <td>Lietvārds</td>\n",
       "      <td></td>\n",
       "      <td>Daudzskaitlis</td>\n",
       "      <td></td>\n",
       "      <td>Nominatīvs</td>\n",
       "      <td>Vīriešu</td>\n",
       "      <td></td>\n",
       "      <td>Sugas vārds</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁tur</td>\n",
       "      <td>Darbības vārds</td>\n",
       "      <td></td>\n",
       "      <td>Nepiemīt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Pagātne</td>\n",
       "      <td>3</td>\n",
       "      <td>Patstāvīgs darbības vārds</td>\n",
       "      <td>Īstenības</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ēja</td>\n",
       "      <td>Darbības vārds</td>\n",
       "      <td></td>\n",
       "      <td>Nepiemīt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Pagātne</td>\n",
       "      <td>3</td>\n",
       "      <td>Patstāvīgs darbības vārds</td>\n",
       "      <td>Īstenības</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁to</td>\n",
       "      <td>Vietniekvārds</td>\n",
       "      <td></td>\n",
       "      <td>Vienskaitlis</td>\n",
       "      <td></td>\n",
       "      <td>Akuzatīvs</td>\n",
       "      <td>Vīriešu</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Norādāmais</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁rokās</td>\n",
       "      <td>Lietvārds</td>\n",
       "      <td></td>\n",
       "      <td>Daudzskaitlis</td>\n",
       "      <td></td>\n",
       "      <td>Lokatīvs</td>\n",
       "      <td>Sieviešu</td>\n",
       "      <td></td>\n",
       "      <td>Sugas vārds</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>Pieturzīme</td>\n",
       "      <td>Punkts</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token       Vārdšķira Pieturzīmes tips       Skaitlis Rekcija    Locījums   \n",
       "0     ▁Br       Lietvārds                   Daudzskaitlis          Nominatīvs  \\\n",
       "1     īdi       Lietvārds                   Daudzskaitlis          Nominatīvs   \n",
       "2    ▁tur  Darbības vārds                        Nepiemīt                       \n",
       "3     ēja  Darbības vārds                        Nepiemīt                       \n",
       "4     ▁to   Vietniekvārds                    Vienskaitlis           Akuzatīvs   \n",
       "5  ▁rokās       Lietvārds                   Daudzskaitlis            Lokatīvs   \n",
       "6       .      Pieturzīme           Punkts                                      \n",
       "\n",
       "     Dzimte Skaitlis 2 Lietvārda tips    Laiks Persona   \n",
       "0   Vīriešu               Sugas vārds                   \\\n",
       "1   Vīriešu               Sugas vārds                    \n",
       "2                                      Pagātne       3   \n",
       "3                                      Pagātne       3   \n",
       "4   Vīriešu                                          3   \n",
       "5  Sieviešu               Sugas vārds                    \n",
       "6                                                        \n",
       "\n",
       "         Darbības vārda tips  Izteiksme Lokāmība Noteiktība Saīsinājuma tips   \n",
       "0                                                                             \\\n",
       "1                                                                              \n",
       "2  Patstāvīgs darbības vārds  Īstenības                                        \n",
       "3  Patstāvīgs darbības vārds  Īstenības                                        \n",
       "4                                                                              \n",
       "5                                                                              \n",
       "6                                                                              \n",
       "\n",
       "  Apstākļa vārda tips Vietniekvārda tips Reziduāļa tips  \n",
       "0                                                        \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                             Norādāmais                 \n",
       "5                                                        \n",
       "6                                                        "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Dataframe to visualize each tag\n",
    "pd.DataFrame(list(zip(tokens, *predicted_tags_list)), columns=[\"token\"]+TAG_NAMES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu3",
   "language": "python",
   "name": "gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
